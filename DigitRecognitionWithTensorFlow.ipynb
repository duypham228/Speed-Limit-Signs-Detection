{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rX8mhOLljYeM"
   },
   "source": [
    "##### Copyright 2018 The TensorFlow Authors.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "BZSlp3DAjdYf"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3wF5wszaj97Y"
   },
   "source": [
    "# Get Started with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DUNzJc4jTj6G"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/_index.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/_index.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hiH7AC-NTniF"
   },
   "source": [
    "This is a [Google Colaboratory](https://colab.research.google.com/notebooks/welcome.ipynb) notebook file. Python programs are run directly in the browser—a great way to learn and use TensorFlow. To run the Colab notebook:\n",
    "\n",
    "1. Connect to a Python runtime: At the top-right of the menu bar, select *CONNECT*.\n",
    "2. Run all the notebook code cells: Select *Runtime* > *Run all*.\n",
    "\n",
    "For more examples and guides (including details for this program), see [Get Started with TensorFlow](https://www.tensorflow.org/get_started/).\n",
    "\n",
    "Let's get started, import the TensorFlow library into your program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0trJmd6DjqBZ"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.tensorflow.org/tutorials/load_data/images\n",
    "\n",
    "#images folder path\n",
    "dirname = os.getcwd()\n",
    "# curpath = os.path.join(dirname, 'dev_ws/dataset/crop-images')\n",
    "alternate = os.path.join(dirname, 'dev_ws/dataset/dataset-ml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 272 files belonging to 10 classes.\n",
      "Using 218 files for training.\n",
      "Found 272 files belonging to 10 classes.\n",
      "Using 54 files for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 11:29:14.780206: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "#https://www.tensorflow.org/tutorials/keras/save_and_load\n",
    "#https://www.pythonfixing.com/2022/01/fixed-batchdataset-get-img-array-and.html \n",
    "\n",
    "#specify image dimensions\n",
    "img_height = 28\n",
    "img_width = 28\n",
    "\n",
    "#load in the dataset with train-test split\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(alternate, validation_split=0.2, subset=\"training\", seed=123,\n",
    "  image_size=(img_height, img_width))\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(alternate, validation_split=0.2, subset=\"validation\", seed=123,\n",
    "  image_size=(img_height, img_width))\n",
    "\n",
    "train_ds_unbatch = train_ds.unbatch()\n",
    "x_train = list(train_ds_unbatch.map(lambda x, y: x))\n",
    "y_train = list(train_ds_unbatch.map(lambda x, y: y))\n",
    "\n",
    "val_ds_unbatch = val_ds.unbatch()\n",
    "x_test = list(val_ds_unbatch.map(lambda x, y: x))\n",
    "y_test = list(val_ds_unbatch.map(lambda x, y: y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine our dataset. Below code was taken from https://github.com/ageron/handson-ml/blob/master/03_classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "def plot_digit(data):\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap = mpl.cm.binary, interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/duy/Desktop/482/Capstone_Speed_Limit_Classification/DigitRecognitionWithTensorFlow.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/duy/Desktop/482/Capstone_Speed_Limit_Classification/DigitRecognitionWithTensorFlow.ipynb#ch0000010?line=0'>1</a>\u001b[0m \u001b[39m# model = tf.keras.models.Sequential([\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/duy/Desktop/482/Capstone_Speed_Limit_Classification/DigitRecognitionWithTensorFlow.ipynb#ch0000010?line=1'>2</a>\u001b[0m \u001b[39m#   tf.keras.layers.Flatten(),\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/duy/Desktop/482/Capstone_Speed_Limit_Classification/DigitRecognitionWithTensorFlow.ipynb#ch0000010?line=2'>3</a>\u001b[0m \u001b[39m#   tf.keras.layers.Dense(512, activation=tf.nn.relu),\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/duy/Desktop/482/Capstone_Speed_Limit_Classification/DigitRecognitionWithTensorFlow.ipynb#ch0000010?line=3'>4</a>\u001b[0m \u001b[39m#   tf.keras.layers.Dropout(0.2),\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/duy/Desktop/482/Capstone_Speed_Limit_Classification/DigitRecognitionWithTensorFlow.ipynb#ch0000010?line=4'>5</a>\u001b[0m \u001b[39m#   tf.keras.layers.Dense(10, activation=tf.nn.softmax)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/duy/Desktop/482/Capstone_Speed_Limit_Classification/DigitRecognitionWithTensorFlow.ipynb#ch0000010?line=5'>6</a>\u001b[0m \u001b[39m# ])\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/duy/Desktop/482/Capstone_Speed_Limit_Classification/DigitRecognitionWithTensorFlow.ipynb#ch0000010?line=7'>8</a>\u001b[0m model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mSequential([\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/duy/Desktop/482/Capstone_Speed_Limit_Classification/DigitRecognitionWithTensorFlow.ipynb#ch0000010?line=8'>9</a>\u001b[0m   tf\u001b[39m.\u001b[39;49mlayers\u001b[39m.\u001b[39mRescaling(\u001b[39m1.\u001b[39m\u001b[39m/\u001b[39m\u001b[39m255\u001b[39m, input_shape\u001b[39m=\u001b[39m(img_height, img_width, \u001b[39m3\u001b[39m)),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/duy/Desktop/482/Capstone_Speed_Limit_Classification/DigitRecognitionWithTensorFlow.ipynb#ch0000010?line=9'>10</a>\u001b[0m   tf\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mConv2D(\u001b[39m16\u001b[39m, \u001b[39m3\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msame\u001b[39m\u001b[39m'\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/duy/Desktop/482/Capstone_Speed_Limit_Classification/DigitRecognitionWithTensorFlow.ipynb#ch0000010?line=10'>11</a>\u001b[0m   tf\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mMaxPooling2D(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/duy/Desktop/482/Capstone_Speed_Limit_Classification/DigitRecognitionWithTensorFlow.ipynb#ch0000010?line=11'>12</a>\u001b[0m   tf\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mConv2D(\u001b[39m32\u001b[39m, \u001b[39m3\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msame\u001b[39m\u001b[39m'\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/duy/Desktop/482/Capstone_Speed_Limit_Classification/DigitRecognitionWithTensorFlow.ipynb#ch0000010?line=12'>13</a>\u001b[0m   tf\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mMaxPooling2D(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/duy/Desktop/482/Capstone_Speed_Limit_Classification/DigitRecognitionWithTensorFlow.ipynb#ch0000010?line=13'>14</a>\u001b[0m   tf\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mConv2D(\u001b[39m64\u001b[39m, \u001b[39m3\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msame\u001b[39m\u001b[39m'\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/duy/Desktop/482/Capstone_Speed_Limit_Classification/DigitRecognitionWithTensorFlow.ipynb#ch0000010?line=14'>15</a>\u001b[0m   tf\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mMaxPooling2D(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/duy/Desktop/482/Capstone_Speed_Limit_Classification/DigitRecognitionWithTensorFlow.ipynb#ch0000010?line=15'>16</a>\u001b[0m   tf\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mFlatten(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/duy/Desktop/482/Capstone_Speed_Limit_Classification/DigitRecognitionWithTensorFlow.ipynb#ch0000010?line=16'>17</a>\u001b[0m   tf\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(\u001b[39m128\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/duy/Desktop/482/Capstone_Speed_Limit_Classification/DigitRecognitionWithTensorFlow.ipynb#ch0000010?line=17'>18</a>\u001b[0m   tf\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(\u001b[39m9\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/duy/Desktop/482/Capstone_Speed_Limit_Classification/DigitRecognitionWithTensorFlow.ipynb#ch0000010?line=18'>19</a>\u001b[0m ])\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'layers'"
     ]
    }
   ],
   "source": [
    "# model = tf.keras.models.Sequential([\n",
    "#   tf.keras.layers.Flatten(),\n",
    "#   tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "#   tf.keras.layers.Dropout(0.2),\n",
    "#   tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "# ])\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "  tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(9)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0423 - accuracy: 0.9954 - val_loss: 76.2982 - val_accuracy: 0.7037\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0490 - accuracy: 0.9862 - val_loss: 82.7161 - val_accuracy: 0.6481\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0428 - accuracy: 0.9725 - val_loss: 63.6346 - val_accuracy: 0.7407\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.9257 - accuracy: 0.9771 - val_loss: 73.2732 - val_accuracy: 0.6667\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.9621 - accuracy: 0.9862 - val_loss: 99.0491 - val_accuracy: 0.6481\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 2.1700 - accuracy: 0.9679 - val_loss: 68.3762 - val_accuracy: 0.6667\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.4056 - accuracy: 0.9725 - val_loss: 63.8435 - val_accuracy: 0.6852\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.9675 - accuracy: 0.9908 - val_loss: 110.1325 - val_accuracy: 0.5926\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 4.0866 - accuracy: 0.9633 - val_loss: 56.3806 - val_accuracy: 0.7222\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 1.1992 - accuracy: 0.9862 - val_loss: 51.1010 - val_accuracy: 0.7222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16212ea00>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, validation_data=val_ds, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T4JfEh7kvx6m"
   },
   "source": [
    "You’ve now trained an image classifier. See [Get Started with TensorFlow](https://www.tensorflow.org/get_started/) to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hsTSsnyfF9U0"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries to handle loading and displaying images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction_and_display_image(np_image, tensorflow_model):\n",
    "    image = np_image.reshape(1, 28, 28)\n",
    "\n",
    "    print(\"Predicted Label: \", np.argmax(tensorflow_model.predict(image)[0]))\n",
    "    print('Actual image: ')\n",
    "    plt.figure(figsize = (1, 1))\n",
    "    plt.imshow(Image.fromarray(image[0]), cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test our model with one random image in our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicted Label: \", np.argmax(model.predict(x_test[42].numpy())))\n",
    "print('Actual image: ')\n",
    "plt.figure(figsize = (1, 1))\n",
    "plt.imshow((x_test[42].numpy()), cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_three = np.array(np.array(Image.open('mnist_test/3.png')))\n",
    "make_prediction_and_display_image(number_three, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OscxG_BhIfa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.Image.Image'>\n",
      "<class 'numpy.ndarray'>\n",
      "Predicted Label:  6\n",
      "Actual image: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x162370c10>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# number_nine = Image.open('mnist_test/dataset-ml/0/0-m-1.jpeg')\n",
    "number_nine = image.load_img('mnist_test/6.png', target_size=(28, 28))\n",
    "print(type(number_nine))\n",
    "img_array = image.img_to_array(number_nine)\n",
    "img_batch = np.expand_dims(img_array, axis=0)\n",
    "img_preprocessed = preprocess_input(img_batch)\n",
    "print(type(img_preprocessed))\n",
    "print(\"Predicted Label: \", np.argmax(model.predict(img_preprocessed)))\n",
    "print('Actual image: ')\n",
    "plt.figure(figsize = (1, 1))\n",
    "plt.imshow(number_nine, cmap = 'gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label:  6\n",
      "Actual image: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1623bea90>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# number_nine = Image.open('mnist_test/dataset-ml/0/0-m-1.jpeg')\n",
    "number_nine = image.load_img('mnist_test/dataset-ml/7/7-m-1.jpeg', target_size=(28, 28))\n",
    "img_array = image.img_to_array(number_nine)\n",
    "img_batch = np.expand_dims(img_array, axis=0)\n",
    "img_preprocessed = preprocess_input(img_batch)\n",
    "print(\"Predicted Label: \", np.argmax(model.predict(img_preprocessed)))\n",
    "print('Actual image: ')\n",
    "plt.figure(figsize = (1, 1))\n",
    "plt.imshow(number_nine, cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label:  6\n",
      "Actual image: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14ded0ac0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# number_nine = Image.open('mnist_test/dataset-ml/0/0-m-1.jpeg')\n",
    "number_nine = image.load_img('mnist_test/47-8.jpeg', target_size=(28, 28))\n",
    "img_array = image.img_to_array(number_nine)\n",
    "img_batch = np.expand_dims(img_array, axis=0)\n",
    "img_preprocessed = preprocess_input(img_batch)\n",
    "print(\"Predicted Label: \", np.argmax(model.predict(img_preprocessed)))\n",
    "print('Actual image: ')\n",
    "plt.figure(figsize = (1, 1))\n",
    "plt.imshow(number_nine, cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 28, 28, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 28, 28, 3), dtype=tf.float32, name='flatten_input'), name='flatten_input', description=\"created by layer 'flatten_input'\"), but it was called on an input with incompatible shape (None, 28, 28).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/keras/engine/training.py\", line 1621, in predict_function  *\n        return step_function(self, iterator)\n    File \"/Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/keras/engine/training.py\", line 1611, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/keras/engine/training.py\", line 1604, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/keras/engine/training.py\", line 1572, in predict_step\n        return self(x, training=False)\n    File \"/Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 247, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \"sequential\" (type Sequential).\n    \n    Input 0 of layer \"dense\" is incompatible with the layer: expected axis -1of input shape to have value 2352, but received input with shape (None, 784)\n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(None, 28, 28), dtype=uint8)\n      • training=False\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/duy/Desktop/482/Capstone_Speed_Limit_Classification/DigitRecognitionWithTensorFlow.ipynb Cell 24'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/duy/Desktop/482/Capstone_Speed_Limit_Classification/DigitRecognitionWithTensorFlow.ipynb#ch0000021?line=0'>1</a>\u001b[0m number_nine \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(np\u001b[39m.\u001b[39marray(Image\u001b[39m.\u001b[39mopen(\u001b[39m'\u001b[39m\u001b[39mmnist_test/another_9.png\u001b[39m\u001b[39m'\u001b[39m)))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/duy/Desktop/482/Capstone_Speed_Limit_Classification/DigitRecognitionWithTensorFlow.ipynb#ch0000021?line=1'>2</a>\u001b[0m make_prediction_and_display_image(number_nine, model)\n",
      "\u001b[1;32m/Users/duy/Desktop/482/Capstone_Speed_Limit_Classification/DigitRecognitionWithTensorFlow.ipynb Cell 17'\u001b[0m in \u001b[0;36mmake_prediction_and_display_image\u001b[0;34m(np_image, tensorflow_model)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/duy/Desktop/482/Capstone_Speed_Limit_Classification/DigitRecognitionWithTensorFlow.ipynb#ch0000016?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_prediction_and_display_image\u001b[39m(np_image, tensorflow_model):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/duy/Desktop/482/Capstone_Speed_Limit_Classification/DigitRecognitionWithTensorFlow.ipynb#ch0000016?line=1'>2</a>\u001b[0m     image \u001b[39m=\u001b[39m np_image\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m28\u001b[39m, \u001b[39m28\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/duy/Desktop/482/Capstone_Speed_Limit_Classification/DigitRecognitionWithTensorFlow.ipynb#ch0000016?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPredicted Label: \u001b[39m\u001b[39m\"\u001b[39m, np\u001b[39m.\u001b[39margmax(tensorflow_model\u001b[39m.\u001b[39;49mpredict(image)[\u001b[39m0\u001b[39m]))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/duy/Desktop/482/Capstone_Speed_Limit_Classification/DigitRecognitionWithTensorFlow.ipynb#ch0000016?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mActual image: \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/duy/Desktop/482/Capstone_Speed_Limit_Classification/DigitRecognitionWithTensorFlow.ipynb#ch0000016?line=5'>6</a>\u001b[0m     plt\u001b[39m.\u001b[39mfigure(figsize \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> <a href='file:///Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     <a href='file:///Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     <a href='file:///Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:1129\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=1126'>1127</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=1127'>1128</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> <a href='file:///Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=1128'>1129</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[1;32m   <a href='file:///Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=1129'>1130</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=1130'>1131</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/keras/engine/training.py\", line 1621, in predict_function  *\n        return step_function(self, iterator)\n    File \"/Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/keras/engine/training.py\", line 1611, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/keras/engine/training.py\", line 1604, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/keras/engine/training.py\", line 1572, in predict_step\n        return self(x, training=False)\n    File \"/Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 247, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \"sequential\" (type Sequential).\n    \n    Input 0 of layer \"dense\" is incompatible with the layer: expected axis -1of input shape to have value 2352, but received input with shape (None, 784)\n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(None, 28, 28), dtype=uint8)\n      • training=False\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "number_nine = np.array(np.array(Image.open('mnist_test/another_9.png')))\n",
    "make_prediction_and_display_image(number_nine, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 28, 28, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 28, 28, 3), dtype=tf.float32, name='flatten_input'), name='flatten_input', description=\"created by layer 'flatten_input'\"), but it was called on an input with incompatible shape (None, 28, 28).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/keras/engine/training.py\", line 1621, in predict_function  *\n        return step_function(self, iterator)\n    File \"/Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/keras/engine/training.py\", line 1611, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/keras/engine/training.py\", line 1604, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/keras/engine/training.py\", line 1572, in predict_step\n        return self(x, training=False)\n    File \"/Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 247, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \"sequential\" (type Sequential).\n    \n    Input 0 of layer \"dense\" is incompatible with the layer: expected axis -1of input shape to have value 2352, but received input with shape (None, 784)\n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(None, 28, 28), dtype=uint8)\n      • training=False\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/duy/Desktop/482/Capstone_Speed_Limit_Classification/DigitRecognitionWithTensorFlow.ipynb Cell 25'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/duy/Desktop/482/Capstone_Speed_Limit_Classification/DigitRecognitionWithTensorFlow.ipynb#ch0000022?line=0'>1</a>\u001b[0m number_nine \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(np\u001b[39m.\u001b[39marray(Image\u001b[39m.\u001b[39mopen(\u001b[39m'\u001b[39m\u001b[39mmnist_test/ugly_9.png\u001b[39m\u001b[39m'\u001b[39m)))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/duy/Desktop/482/Capstone_Speed_Limit_Classification/DigitRecognitionWithTensorFlow.ipynb#ch0000022?line=1'>2</a>\u001b[0m make_prediction_and_display_image(number_nine, model)\n",
      "\u001b[1;32m/Users/duy/Desktop/482/Capstone_Speed_Limit_Classification/DigitRecognitionWithTensorFlow.ipynb Cell 17'\u001b[0m in \u001b[0;36mmake_prediction_and_display_image\u001b[0;34m(np_image, tensorflow_model)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/duy/Desktop/482/Capstone_Speed_Limit_Classification/DigitRecognitionWithTensorFlow.ipynb#ch0000016?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_prediction_and_display_image\u001b[39m(np_image, tensorflow_model):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/duy/Desktop/482/Capstone_Speed_Limit_Classification/DigitRecognitionWithTensorFlow.ipynb#ch0000016?line=1'>2</a>\u001b[0m     image \u001b[39m=\u001b[39m np_image\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m28\u001b[39m, \u001b[39m28\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/duy/Desktop/482/Capstone_Speed_Limit_Classification/DigitRecognitionWithTensorFlow.ipynb#ch0000016?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPredicted Label: \u001b[39m\u001b[39m\"\u001b[39m, np\u001b[39m.\u001b[39margmax(tensorflow_model\u001b[39m.\u001b[39;49mpredict(image)[\u001b[39m0\u001b[39m]))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/duy/Desktop/482/Capstone_Speed_Limit_Classification/DigitRecognitionWithTensorFlow.ipynb#ch0000016?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mActual image: \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/duy/Desktop/482/Capstone_Speed_Limit_Classification/DigitRecognitionWithTensorFlow.ipynb#ch0000016?line=5'>6</a>\u001b[0m     plt\u001b[39m.\u001b[39mfigure(figsize \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> <a href='file:///Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     <a href='file:///Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     <a href='file:///Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:1129\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=1126'>1127</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=1127'>1128</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> <a href='file:///Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=1128'>1129</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[1;32m   <a href='file:///Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=1129'>1130</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=1130'>1131</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/keras/engine/training.py\", line 1621, in predict_function  *\n        return step_function(self, iterator)\n    File \"/Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/keras/engine/training.py\", line 1611, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/keras/engine/training.py\", line 1604, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/keras/engine/training.py\", line 1572, in predict_step\n        return self(x, training=False)\n    File \"/Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/duy/miniforge3/envs/TFmacOS/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 247, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \"sequential\" (type Sequential).\n    \n    Input 0 of layer \"dense\" is incompatible with the layer: expected axis -1of input shape to have value 2352, but received input with shape (None, 784)\n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(None, 28, 28), dtype=uint8)\n      • training=False\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "number_nine = np.array(np.array(Image.open('mnist_test/ugly_9.png')))\n",
    "make_prediction_and_display_image(number_nine, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label:  8\n",
      "Actual image: \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFkAAABYCAYAAACeV1sKAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAIZUlEQVR4nO2cW2hU2xnHf99MxkzESmrCiLRq4g2JRg04BU0UFELqMZcKeuwRYyKIffBAxT5UBCHgS5C2GAQLKSqnaGjAivGttAcfjHiLmtjkHFJPTKTnYEwmim1GTeby9SGTaWJmJpO5bGeS/YPFZNZea+0v//n49rrtJaqKSXKxfGoD5gKmyAZgimwApsgGYIpsAKbIBhCXyCLycxHpFpHvRORkooyabUis/WQRsQL/AkqB74GHwBeq+k3izJsdZMRR92fAd6r6HEBE/gJUAWFFFpFZO/JRVQl3LZ5w8RPg3xO+fx/Im4SIHBWRNhFpi+NeaU08nhwVqtoINMLs9uRIxOPJPwBLJ3z/aSDP5CPiEfkhsFpE8kVkHvBL4GZizJpdxBwuVNUrIl8CfwOswCVV7UqYZVHicDjIy8ubttyLFy949epV8g0KQcxduJhuloSYfPjwYerr6xEJ+3BHVTl9+jSNjY2Jvv3Ee4Q1IOkPvmSxePFiVq5cSWFhIQ6HY9ry69ato7i4eFKeqvLs2TMGBweTZeb/b2RUAjRRqbq6Wl0ul7rdbo2Gd+/e6evXryelgYEB3bt3b0LsifR/p40nOxwOCgoKgmFh48aNLFq0KGKYmEhWVhZZWVmT8nw+H0VFRbhcLrq6upLn0eniyXv27NHBwUF9+/atvn37Vt+/fx+VB0fj4YODg1pZWTl3PdnhcLBp0yacTifZ2dlkZEQ2+fnz53R3d7N69WpWrVo1bftZWVlYrVZsNluiTJ5Cyk91Op1OmpqaOHHixLQCA1y/fp19+/Zx7do1A6yLjpT1ZIfDgdPpZOvWrSxcuHBaT+vp6aGrq4uOjg7cbjdPnz6lpaVlUhkRYcOGDVP61RaLBafTycjICA8fPkx8fzpVY3JpaakODQ3p6OhoVLG1oaFB7Xa7ZmRkKKAZGRlqt9snpaysLG1sbAxZf3R0VN+8eaO7d++eOzHZYrFgt9vDenBvby9tbW3jPx6PHj3iw4cPweterxev1zul3oMHD8jOzmbz5s3k5+cH8202G5mZmVgsSYigqerJZWVlEfvAly5dUrvdrjabTW02m1qt1qjatVqtarfb9fLly1PafPfunVZUVMwdTw5Hb28vd+7c4fbt24yMjAQ9OVp8Ph+qit/vT5KFU0k7ke/evcuRI0fweDwzFvhTkXYir1ixgpqaGnw+HwBdXV3cu3cv6vrFxcUUFBSwZs2aZJk4lXSMyT6fL5jOnz+vgdm9aZOI6IULF9Tn84Vs14zJE5jYA9iwYQPHjh2LKnSICIWFhcnpQUQgLUWeyPbt29m2bVvU5aOdUEokKTus7uvro6GhgZaWlmD8DYeIRJ0+Cakak8dTVVVVwmbcpmPOxuTu7m7Onj0bHPmtX7+e8vLyT+eVsZAMjw2XiMFDPk4HDx5Ur9drenIyaW9vp66uLq4egohQVVVFUVFRAi0LT9qJ3NnZSWdnZ1xtWCwWli1bZpjIKdu7mE2YIhuAKbIBmCIbgCmyAcw5kUUEq9Vq6GAm7bpw8VJdXc3OnTvZunWrYfdMWZHHPS4cfr9/xktIIsKWLVuoqamJ17wZMa3IIrIU+DOwmLEhZKOqNojIIqAZyAP6gM9V9U2iDCssLOT48eNkZmaGvH7r1i0uXryYFktQ0XiyF/iNqj4WkR8Bj0Tk70At8LWq1gfe4TsJ/DZRhi1ZsoT9+/czf/78kNdHRka4evUqHo8n5NL/TPF6vYyOjiZlgXXaB5+qvlTVx4G//wt8y9hbTlXAV4FiXwG/SLh1EdixYwdXrlyhtrY27ra8Xi/nzp2jtraWx48fx2/cR8woJotIHlAE3AcWq+rLwKV+xsJJqDpHgaNx2BiSvLw88vLyGBwcpKmpCY/Hg8fjCVveZrNhs9lC7qfz+/3cu3ePGzduJNpMYAZdOBFZAPwVOK6q/5l4TccCY8jgqKqNqrpZVTfHZWkYysrKaG5u5tChQxHL1dTU0NzcTGlpaTLMiEhUniwiNsYEvqqq1wPZr0Rkiaq+FJElwEAiDfP5fAwPDwe3a4Vj3KP7+vpYsGBB2HJFRUWUl5cn0sSoiaZ3IcBF4FtV/cOESzeBGqA+8NkSonrMPHnyhAMHDlBSUsKpU6eYN29exPIVFRUR91KsXbs2kebNjChWM0oYCwVPgfZA+gzIAb4GngH/ABYlY2Vk165d2t/fH/W7ITPF7XbrwMBAUnfap/zyU05OjpaUlGhdXZ16PJ6ECuz1evXMmTNaUlKiubm5SRM5ZUd84wwNDdHa2kpOTg4ulyu4oJqZmRkxBk/H8PAwbrebjo4OWltbE2VuSNLmZcmcnBxWrVoVnNiprKzk5MmTMU30+P1+6uvruXnzJj09PbhcrljNCqKz4WXJoaEhhoaGgt/z8/Pp7++PaUHV7/fT2dnJ/fv3E2liWNLGkz8mNzeX5cuXx1y/r69v0o8WL5E8OW1FTjUiiTznJu0/BabIBmCKbACmyAZgdBfOBbgDn+lKLlPtj9jNMbR3ASAibZqkaU8jiMV+M1wYgCmyAXwKkZN32pIxzNh+w2PyXMQMFwZgimwAhomcjgdai8hSEbklIt+ISJeI/DqQXyciP4hIeyB9FrEdI2Jyuh5oHViFX6ITdk8xtonnc2BYVX8XTTtGeXLwQGtVHQXGD7ROaTT87qkZYZTIUR1oncp8tHsK4EsReSoil0Tkx5Hqmg++KAixe+qPwEpgE/AS+H2k+kaJnLYHWofaPaWqr1TVp6p+4E+MhcOwGCVyWh5oHW73VOCBOM4eIOLbm4ZMdWqKHGgdA8VANfBPEWkP5J0CvhCRTYxtbOkDfhWpEXNYbQDmg88ATJENwBTZAEyRDcAU2QBMkQ3AFNkA/gdSZxNJIAC3PQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "number_nine = np.array(np.array(Image.open('mnist_test/one_more_9.png')))\n",
    "make_prediction_and_display_image(number_nine, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label:  8\n",
      "Actual image: \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "number_nine = np.array(np.array(Image.open('mnist_test/nine.png')))\n",
    "make_prediction_and_display_image(number_nine, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9GmHIZI4L1kz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label:  8\n",
      "Actual image: \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "number_three = np.array(np.array(Image.open('mnist_test/3.png')))\n",
    "make_prediction_and_display_image(number_three, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jgEonSJ9MR7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label:  8\n",
      "Actual image: \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFkAAABYCAYAAACeV1sKAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAGbUlEQVR4nO2cX2hTVxyAv1+6xiJKpxtIs6WbDFFGrc4WIU77pDgU0UAd6UPxya6IYNmT7mkvChvbUCEdOqdPgyHVMvVlbEUG8aHqZu1WNU7GiIobTDbXBUr+/faQtFTb3Kb5c5pbzweH5p6be+4vX07PuTfn3COqiqWyeOY6gOcBK9kAVrIBrGQDWMkGsJINUJJkEXlHRKIick9EDpYrqPmGFHudLCI1wF1gC/AAuAZ0qOqt8oU3P3ihhGPXA/dU9TcAEfka2AnklSwi8/bOR1Ul375SmotXgPuTth/k8p5CRLpE5LqIXC/hXK6mlJpcEKp6EjgJ87smO1FKTX4I+Cdtv5rLszxDKZKvAStEZLmIeIEQcKE8Yc0vim4uVDUlIvuBb4Ea4LSqjpQtsnlE0ZdwRZ1sHrfJTlcXFe/4TOHz+Vi+fHlRx6oq0WiUx48flzmqLPNG8s6dOzl8+HBRxyYSCfbu3cvFixfLHFUW10puaGhg1apVE9urV69myZIlRZWVSCTwer3lCm0KrpW8ZcsWjh07hseTvUBasGDBHEeUH9dJ9vl8NDc3s27dOurr6xHJ299UDa6T3NbWxokTJ6irq3OFYHCRZJ/PR0tLC+vXr2fRokUTzUQ+otEod+7cKajsVCrFw4eVu1l1jeRAIMCZM2eoq6ubUTDA2bNnOXLkSEFlqyrJZLLUEPNS9ZJ9Ph+BQICNGzeycOFCampqpn3f3bt3GRoamti+efMmY2NjhqKcAVU1lgCdbdqxY4c+efJEU6mUOnH06FH1er1aW1urtbW16vF4Zn2uUpLT5676muzxePB6vXlrcDQa5erVqwwODpJIJAxHVxhVL3kmBgYG6OnpIZ1Oz3UoeXGt5Gg0ypUrV4hEIhXttMqBayVHIhG6u7urugaP4+p5F5lMZrxDrWpcLdktWMkGsJIN4NqOb82aNRw6dIhMJlN0GZlMhkuXLjEyUtmhSddKbm1tpbW1taQyUqkUsVjMSq4kHo+H3bt3s2LFCvr6+rh1qzLT+J57ycFgkO3btzMyMlIxybbjM4CVbADXNxeRSIRz584V9F4Rob29nQ0bNlQ4qqdxveTh4WGOHz/ueCk3Phbo8XhYuXKllTxb2traCIfDDAwM0NfXN2X/rl272Lp168T2pk2bTIYHuECyqpJKpVDVaUenm5qaaGpqYmxsjP7+/ik/GAUCAbq7u02FOy1VL/nGjRvs27ePQCBAV1dX3hGSzZs3c+rUqSn5LS0tlQ5xZqp9jG88tbe3azwe13Q67TjWN1vS6bTG43ENBoMVG+MrRIwfuEz2gZsR4EAufynwHfBr7u+SSkpubGzUUCikvb29ZROdTqc1HA5rKBRSv98/p5IbgHW514vJPlb2JvAxcDCXfxD4qJKSx1NHR4fG43FNJpMlCU4mkzo6OqqhUKjio9XF/Mt/Q/bZvSjQMOmLiJqQ3NjYqMFgUHt7ezWTyRQlOJPJaDgc1mAwWHINLkTyrDo+EXkdeAsYBJap6qPcrj+AZXmO6QK6ZnMeJ2KxGLFYjPr6ejo7O/N2hE6k02kGBwfp7+8vV1iOFPw4g4gsAn4ADqvqeRH5R1VfnLT/b1V1nCBczscZ/H4/zc3NRU06VFWGh4e5f//+zG8uvMz8gRTYRNSSfQDn/Ul5c9JcVGty+twz/kAk2aryJXBbVT+btOsCsCf3eg/ZttoyHQXUvo1kv61hYCiXtgEvAQNkL+G+B5bamjx9so+YlQmnNtn+nmwAK9kAVrIBrGQDWMkGsJINYCUbwEo2gJVsANNjfH8B8dxft/IyU+N/zekAo7fVACJyXVVLm445hxQTv20uDGAlG2AuJJ+cg3OWk1nHb7xNfh6xzYUBrGQDGJPsxgWtRcQvIpdF5JaIjIjIgVz+hyLyUESGcmmbYzkm2mS3LmgtIg1kR+R/EpHFwI/ALuBd4D9V/aSQckzV5IkFrVU1AYwvaF3VqOojVf0p93oUuM00a0TPhCnJBS1oXc08M3sKYL+IDIvIaRFxnNRjO74CyM2eOgf0qOq/wOfAG8Ba4BHwqdPxpiS7dkFrEaklK/grVT0PoKp/qmpaVTPAF2Sbw7yYkuzKBa3zzZ7KdYjjBIFfnMox8lOnundB67eBTuBnERnK5X0AdIjIWrKzh34H3nMqxN5WG8B2fAawkg1gJRvASjaAlWwAK9kAVrIB/gfn7KYB3Ko3hQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "number_six=np.array(np.array(Image.open('mnist_test/6.png')))\n",
    "make_prediction_and_display_image(number_six, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ADAoS5T1Mn8r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label:  8\n",
      "Actual image: \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "number_five = np.array(np.array(Image.open('mnist_test/number_five.png')))\n",
    "make_prediction_and_display_image(number_five, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = os.getcwd()\n",
    "checkpoint_path = \"training_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.join(dirname, \"training_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cp.ckpt.data-00000-of-00001', 'checkpoint', 'cp.ckpt.index']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 08:46:48.922028: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dev_ws/saved_model/my_model/assets\n"
     ]
    }
   ],
   "source": [
    "# Save the entire model as a SavedModel.\n",
    "model.save('dev_ws/saved_model/my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 2352)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               1204736   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,209,866\n",
      "Trainable params: 1,209,866\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model = tf.keras.models.load_model('dev_ws/saved_model/my_model')\n",
    "\n",
    "# Check its architecture\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 - 3s - loss: 0.4280 - accuracy: 0.9275 - 3s/epoch - 8ms/step\n",
      "Restored model, accuracy: 92.75%\n",
      "(10864, 10)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the restored model\n",
    "loss, acc = new_model.evaluate(val_ds, verbose=2)\n",
    "print('Restored model, accuracy: {:5.2f}%'.format(100 * acc))\n",
    "\n",
    "print(new_model.predict(val_ds).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of _index.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "https://github.com/tensorflow/docs/blob/master/site/en/tutorials/_index.ipynb",
     "timestamp": 1561490564528
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
